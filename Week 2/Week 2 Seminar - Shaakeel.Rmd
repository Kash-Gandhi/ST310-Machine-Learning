---
title: "Week 2 seminar"
author:
  - Prof. Joshua Loftus (lecturer)
  - Shakeel GAvioli-Akilagun (GTA)
output: html_document
---

```{r setup, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Start by loading the usual packages: 

```{r, echo = FALSE}
library(tidyverse)
library(gapminder)
library(broom)
```

# Some useful exercises from the lectures

Prove the regression lines passes through the mean. That is: 

$$
\overline{y} = \sum_{j=1}^p \hat{\beta}_j\overline{x}_j  
$$

Prove the conditional expectation minimizes the squared error loss. That is $E(Y|X) = g^*(X)$ where $g^*$ is given as follows: 

$$
g^*(x) = {\arg\min}_{g(X)}  E \{ (Y - g(X))^2 \} 
$$


# Linear regression

## Simple linear regression

### Estimation

We will use the example from the previous seminar of regressing life expectancy on GDP per capita using the Gap Minder dataset. 

```{r}
gapm_2007 <- gapminder %>% filter(year == 2007)

lm_2007 <- lm(lifeExp ~ gdpPercap, data = gm2007)
lm_2007
```


### Demo dplyr::summaries function

In a simple linare regression model we have the following facts which allows us to compute the intercept and slope from the data directly: 

* Slope = $cor(x,y) * sd(y) / sd(x)$
* Regression line passes through $(mean(x), mean(y))$

```{r}
gapm_2007 %>%
  summarize(cor_xy = cor(gdpPercap, lifeExp),
            sd_x = sd(gdpPercap),
            sd_y = sd(lifeExp),
            xbar = mean(gdpPercap),
            ybar = mean(lifeExp),
            hat_beta1 = cor_xy * sd_y / sd_x,
            hat_beta0 = ybar - hat_beta1 * xbar)
```

### Inference

One of the *selling points* of statistics over other disciplines which develop methods for analysing data (applied maths, computer science, etc.) is the statistician's ability to quantify the uncertainty regarding the procedure applied to the data. 

How can we do this in base R for our linear model?  

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i  
$$

```{r}
summary(lm_2007)
```


The poulation quantitiies are given by (ISLR 3.8): 

$$
se(\hat{\beta}_1) = \frac{\sigma}{\sqrt{\sum_j ( x_j - \overline{x} )^2}}
$$
and

$$
se(\hat{\beta_0}) = \sigma \left ( \frac{1}{n} + \frac{\overline{x}^2}{\sum_j (x_j + \overline{x})^2 } \right )^{\frac{1}{2}}
$$

We will be interested in quantifying uncertainty about the slope. The sample counterpart is given given by: 


$$
\widehat{se(\beta_0)} = \frac{\hat{\sigma}}{\sqrt{\sum_j ( x_j - \overline{x} )^2}} 
$$

with $\hat{\sigma}$ given by (ISLR 3.15): 

$$
\hat{\sigma} = RSE = \sqrt{ RSS / (n-2) } 
$$

We can compute the estimated standard error directly from the `lm` object using the `summarize` command: 

```{r}
augment(lm_2007) %>%
  summarize(RSS = sum(.resid^2),
            RSE = sqrt(RSS / (n() - 2)),
            std.error = RSE / sqrt(sum( (gdpPercap - mean(gdpPercap))^2 ))  )
```

### Model diagnostics

#### The $R^2$

The $R^2$ is generally interpreted as the "proportion of variance in outcome **explained** by simple linear regression model", i.e.

$$
R^2 = \text{cor}(x,y)^2
$$

We can compute this quantity from the data: 

```{r}
cor(gapm_2007$gdpPercap, 
    gapm_2007$lifeExp
    ) ** 2 
```

Alterbatively, the $R^2$ can be written as: 

$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} 
$$

Again, we can compute this quantity from the data: 

```{r}
augment(lm_2007) %>%
  summarize(RSS = sum(.resid ** 2),
            TSS = sum( (lifeExp - mean(lifeExp)) ** 2),
            R2 = 1 - RSS/TSS)
```


### Diagnostic plots

Idea: look for patterns in residuals, which could indicate systemic error (bias)

```{r}
augment(lm_2007) %>%
  ggplot(aes(x = gdpPercap, y = .resid)) +
  geom_point()
```

Other diagnostics which can be checked automatically in base R: 

**QQ Plot** Checking for (approximate) normality with quantile-quantile plot

**Leverage** weighted distance between $x_i$ and the mean of all $x$'s


```{r}
plot(model_lm)
```

[**Cook's distance**](https://en.wikipedia.org/wiki/Cook%27s_distance), `cooksd` in the plots, measures how much the predictions for all other observations change if we leave out one observation

```{r}
library(GGally)
ggnostic(lm_2007)
```

Finally, we can visualize confidence intervals around fitted parameters with the `ggcoef` function: 

```{r}
ggcoef(lm_2007)
```

**Question**: with flexible models, are influential observations more or less harmful, and in which ways?

## Multiple regression

```{r}
library(fivethirtyeight)
head(candy_rankings)
```


### Estimation

```{r}
lm_candy <- lm(
  formula = winpercent ~ sugarpercent + chocolate + fruity,
  data = candy_rankings
)
```


### Inference

```{r}
summary(lm_candy)
```

Hint: plot confidence intervals with `ggcoef` in `GGally` package

```{r}
ggcoef(lm_candy)
```

```{r}
ggnostic(lm_candy)
```

### Diagnostics

e.g. `ggpairs` shows many 2-dimensional projections of the data, but there is no guarantee that these projections together help us understand higher dimensional relationships... including possibly higher dimensional patterns in the residuals

```{r}
ggpairs()
```

### Problem: high dimensional plots...

**Question**: What does this mean for diagnostic plots when our regression model is high dimensional (e.g. p > 3 predictors)
