---
title: 'Simple Regression: Gapminder'
author: "Kash"
date: "11/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(gapminder)
library(broom)
```

## Estimation

```{r}
gm2007 <- gapminder %>% filter(year==2007)
model_lm <- lm(lifeExp ~ gdpPercap, data = gm2007)
model_lm
```

Now, let's try to do this ourselves without relying on the lm function

$$
\hat \beta_1 = \text{cor}(x,y) \frac{\text{sd}(y)}{\text{sd}(x)}
$$

You take the correlation between the 2 variables and then scale it by ratio of standard deviations of outcome and predicted variable. 
The sd of outcome var is in numerator because if the outcome variable has higher sd, then the slope will have to be steeper.
The denominator has the sd of var on horizontal axis. This makes sense because if you rescale your horizontal axis by a factor greater than 1 ie. more deviation (expand horizontally), your slope would become flatter.


Remember, the regression line passes through point that has the mean of x and y as its coordinates.
ie. Regresion line passes through 
$$
(\bar x, \bar y)
$$
This means that:

$$
\bar y = \hat \beta_0 + \hat \beta_1 \bar x
$$

Here, beta hat is the intercept.

```{r}
gm2007 %>% summarize(cor_xy = cor(gdpPercap, lifeExp),
                     sd_x = sd(gdpPercap),
                     sd_y = sd(lifeExp),
                     xbar = mean(gdpPercap),
                     ybar = mean(lifeExp),
                     hat_beta1 = cor_xy * sd_y / sd_x,
                     hat_beta0 = ybar - hat_beta1 * xbar)
```

If you want to do this for the entire dataset:

```{r}
gapminder %>%
  group_by(year) %>%
  summarize(cor_xy = cor(gdpPercap, lifeExp),
                     sd_x = sd(gdpPercap),
                     sd_y = sd(lifeExp),
                     xbar = mean(gdpPercap),
                     ybar = mean(lifeExp),
                     hat_beta1 = cor_xy * sd_y / sd_x,
                     hat_beta0 = ybar - hat_beta1 * xbar)
```


notice that beta hat 1 is not consistent. So, the relation is not consistenmt across the years!

## Inference

```{r}
summary(model_lm)
tidy(model_lm)
```

(ISLR 3.8)

$$
\text{SE} (\hat \beta_1) = \sqrt{ \frac{\sigma^2}{\sum{}(x_i - \bar x)^2}}
$$

where (ISLR 3.15)
$$
\hat \sigma = \text{RSE} = \sqrt {\frac{\text{RSS}}{n-2}}
$$

```{r}
augment(model_lm) %>%
  summarize(RSS = sum(.resid^2),
            RSE = sqrt(RSS/(n()-2)),
            std.error = RSE/sqrt(sum( (gdpPercap - mean(gdpPercap))^2 ))
            )

```


# Model Diagnostics

Our inferences are only as good as the assumptions of our model are true.
Check the assumptions of linear model hold or not.

```{r}
summary(model_lm)
```
Check R-squared here. Can use summary() or glance() for this. 
In simple regression,
$$
R^2 = \text{corr}(x,y)^2
$$
so we can also calculate it manually as follows:
```{r}
cor(gm2007$gdpPercap, gm2007$lifeExp)^2
```
In general, the formula for R-sqaured is:
$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}
$$
where Total Sum of Sqaures ios just the sum of sqaures of y.

```{r}
augment(model_lm) %>%
    summarize(RSS = sum(.resid^2),
              TSS = sum((lifeExp - mean(lifeExp))^2),
              R2 = 1 - RSS/TSS)
```

## Idea: look for patterns in residuals

The errors can be decomposed into parts of errors that are systemic and parts that are not systemic. The systemic parts are related to BIAS and the non-systemic parts are related to variance.

We are mainlky worried about bias in residuals so we will plot them and check.

```{r}
augment(model_lm) %>%
  ggplot(aes(gdpPercap, .resid)) +
  geom_point()
```

It is clearly visible that there is a pattern here.
Data points are concentrated towards the left and then as you move towards the righyt, there is a downward trend.
Sometimes, some trends are not visible in a 2D plot, you'll need higher dimensional plot. So, the this technique of looking for patterns, is only as powerful as our ability to look for patterns.







