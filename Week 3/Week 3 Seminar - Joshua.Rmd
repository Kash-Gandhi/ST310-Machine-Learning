---
title: "Week 3 Seminar"
author: "Kash"
date: "07/01/2022"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(gapminder)
library(modeldata)
data("attrition")
```

# Logistic Regression

## Categorical Outcome Data

Look at the data
```{r}
head(attrition)
```

Compare the distribution of a numeric predictor variable between the 2 outcome classes:
```{r}
ggplot(attrition, aes(x = TotalWorkingYears, y = Attrition)) +
  geom_boxplot()
```

Test for difference in means:
```{r}
t.test(TotalWorkingYears ~ Attrition, data = attrition)
```


Check for class balance:
```{r}
table(attrition$Attrition) # base r

# OR

attrition %>% count(Attrition) 
```

We can clearly see that there is class imbalance here.


  
Create a balanced dataset witht the same number of observations in both classes:

  1. Bring down the number of observations in the larger class
          -ve: It wont be representative of the real world anymore
          -ve: Downsampling adds more weight on the sample datapoints but the issue is more severe with upsampling
          
  2. Bring up the number of observations in the smaller class by sampling with replacement
          -ve: It wont be representative of the real world anymore
          -ve: Upsampling will add more weight on the sample data points since there are so few of them already
          -ve: Upsampling means we are creating false data - counting some datapoints  more than once
          
          
DOWN-SAMPLING:
```{r}
attr_No <- attrition %>%
  filter(Attrition == "No") %>%
  sample_n(size=237)

attr_yes <- attrition %>%
  filter(Attrition == "Yes")

attr <- rbind(attr_No, attr_yes)

# OR

attr <- attrition %>%
  group_by(Attrition) %>%
  slice_sample(n = 237)
```

UP-SAMPLING:
```{r}
attr_up <- attrition %>%
  group_by(Attrition) %>%
  slice_sample(n=1000, replace = TRUE)
```

```{r}
# transform outcome to numeric 0-1
nattr <- attr %>% 
  mutate(Y = as.numeric(Attrition) - 1) %>%
  select(Y, TotalWorkingYears)
```



## CLassification: Linear Regression?

Plot linear regression line, use `geom_hline` to show a classification cutoff rule
```{r}
ggplot(nattr, aes(x = TotalWorkingYears, y = Y)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  geom_hline(yintercept = .4) # or e.g. mean(nattr$Y)
```

Problems:
  - Can predict outside 0-1 range
  - Not directly interpretable as probabilities

We can do linear regression when we have a binary outcome variable. It is called Linear probability model.
However, on future data, you could predict values that are larger than 1 or less than 0.
So, use logistic regression for binary classification.


### Threshloding ideas

Choose a threshold/cutoff value for predictor X, say c, and then classify
  - $\hat Y = 1 $ if $X \geq c$
  - $\hat Y = 0 $ if otherwise
  
Or if the association is negative, change the sign

As we vary $c$, we trade-off between kinds of errors: flase positives and false negatives

In the simple case with thresholding one predictor, the classification/decision rules are all equivalent whether we use linear regression or logistic regression (as long as the fitted relationship is monotone)

For **multiple** regressionâ€“when we have more predictors -- we can then transform a numeric prediction from the model $\hat Y$ to a classification by using a threshold rule on the scale of the predictions (instead of on the scale of one predictor as before)

  - $\hat Y = 1$ if $x^T \hat \beta \geq c$
  - $\hat Y = 0$ otherwise
  

## Logistic Regression

```{r}
glm(Attrition ~ TotalWorkingYears, data = nattr, family = "binomial")
```

Another method:
```{r}
model_glm <- glm(Y ~ TotalWorkingYears, data = nattr, family = "binomial")
```

```{r}
augment(model_glm, type.predict = "response") %>%
  ggplot(aes(TotalWorkingYears, Y)) + 
  geom_point() +
  geom_line(aes(y = .fitted))
```

### Model Assumptions

Do you remember an equation describing the logistic regression model?

$$
\text{logit}[P(Y=1|X)] = \beta_0 + \beta_1 X
$$
$$
P(Y=1|X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e ^ {\beta_0 + \beta_1 X}}
$$
### Interpreting Coefficients

```{r}
exp(coef(model_glm))
# coef are not easily interpretable here so we use exp()
```
Interpretation: If I increase TotalWorkingYears by 1 unit, then odds of attrition are 6.1% lower (100 - 93.9)

### Inference 
Confidence intervals
```{r}
exp(confint(model_glm))
```

Model evaluation measures

```{r}
summary(model_glm)
```

```{r}
glance(model_glm)
```
diagnostic plot: can do this but less common, harder to interpret deviance residuals

(Warning: residual plots almost always show patterns, can't be interpreted the same way as for linear regression)


## Balance and (re)caliberation
What portion of data are classified using a given cutoff for the predicted probability?
```{r}
predict(model_glm)
# dont know how to interpret since they are the log odds of attrition

predict(model_glm, type = "response") # this gives answer in terms of probabilities
```

Now, we need to set a cutoff:
```{r}
mean(predict(model_glm, type = "response") > 0.5)
```

Another method:
```{r}
augment(model_glm, type.response = "response")
```
.hat should be the probabilities ?



Earlier we said that both down-sampling and up-sampling has this problem that they no longer represent the real world. Setting this cut-off is one of the ways to deal with this mismatch.

The purpose of balancing our classes is so that the model can use the most anount of information from the predictor variables to separate the classes and then we can get to whatever kind of class balance we want in our predicitions by moving the threshold somewhere.

Try a different cut-off:
```{r}
c(.3, .4, .5, .6, 7) %>%
  map_dbl(function(cutoff) mean(predict(model_glm, type = "response") > cutoff))
```

Tabulate a confusion matrix using dplyr functions
```{r}
conf_mat <- function(fitted_glm, cutoff = .5) {
  augment(fitted_glm, type.predict = "response") %>%
    mutate(Yhat = as.numeric(.fitted > cutoff)) %>%
    count(Y, Yhat)
}

conf_mat(model_glm)
```

Try another cutoff:
```{r}
conf_mat(model_glm, .6)
```

## Multiple Regression Model

Pick a few predictors and repeat the above
```{r}
library(GGally)
```

```{r}
options(warnings= -1)
attrition %>%
  select(Attrition, Age, Gender, TotalWorkingYears,
         DistanceFromHome, JobSatisfaction) %>%
  ggpairs(progress = FALSE, lower = list(bins = 20))
```


### Model Assumptions:
$$
logit[P(Y=1|X)] = X \beta
$$
Matrix multiplication has an intercept if X has a constant column

## Simulation
We write a function like `y = f(x) + noise` to generate data (with sample size as an input to the function)

Start with a linear function and gaussian noise:
```{r}
f <- function(x) 1 - 2*x
my_dgp <- function(n) {
  x <- runif(n)
  noise <- rnorm(n)
  y <- f(x) + noise
  return(data.frame(x = x, y = y))
}
```

Simulate one dataset with a sample size of 20, fit a linear regression model, and extract the slope estimate

```{r}
coef(lm(y ~ x, data = my_dgp(n = 20)))
coef(lm(y ~ x, data = my_dgp(n = 20)))[2]
```

Repeat this 100 times using replicate(), plot a histogram of the coefficient estimates, add a geom_vline at the true coefficient value. Increase the sample size and try again
```{r}
replicate(5, coef(lm(y ~ x, data = my_dgp(n = 20)))[2])

replicate(1000, coef(lm(y ~ x, data = my_dgp(n = 20)))[2]) %>%
  qplot() + geom_vline(xintercept = -2)
```

### Complexify the above

Now try making `f` a function of two variables, where `x1` and `x2` are both (possibly noisey) functions of a hidden variable `u`

What are the true coefficients? How do we interpret them? What happens if we regress on only `x1` or only `x2`? What would be the change in outcome if we could intervene on `x1` (erasing the influence of `u` on `x1` but keeping it for `x2`)? What if we could intervene on `u`?
```{r}
beta1 <- -1
beta2 <- -2
f <- function(x1, x2) 1 + beta1 * x1 + beta2 * x2
my_dgp <- function(n) {
  u <- rnorm(n)
  x1 <- 3*u + rnorm(n)
  x2 <- 15 - 7*u + rnorm(n)
  y <- f(x1, x2) + rnorm(n)
  return(data.frame(x1 = x1, x2 = x2, y = y))
}
```

```{r}
replicate(100, coef(lm(y ~ x1 + x2, data = my_dgp(n = 1000)))[2]) %>%
  qplot() + geom_vline(xintercept = beta1)
```

```{r}
replicate(100, coef(lm(y ~ x1, data = my_dgp(n = 1000)))[2]) %>%
  qplot() + geom_vline(xintercept = beta1)
```

If we regress on only one at a time the estimates can be biased (omitted variable bias)

If we could intervene on `x1` directly then `beta1` (the true value, not the estimate) would tell us the causal effect on the outcome

If we could intervene on `u` then the total effect on `y` would involve both `beta1` and `beta2`, as well as the coefficients of `u` on both `x1` and `x2`

e.g. increasing `u` by 1 would make `x1` increase by 3 and `x2` decrease by 7, hence `y` would change by `3*beta - 7*beta2`

## Extra Practice

Repeat the previous simulations but generate a binary outcome and use logistic regression to estimate coefficients
```{r}
# y <- rbinom(n, 1, exp(f(x))/(1+exp(f(x))))
```

