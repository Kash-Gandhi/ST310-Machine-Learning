---
title: "Week 4 seminar"
author:
  - Prof. Joshua Loftus (lecturer)
  - Shakeel GAvioli-Akilagun (GTA)
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Please load these packages before we start: 

```{r, message=FALSE, warning=FALSE}
library(tidyverse) 
library(broom)     
library(modelr)    
library(GGally)
```

**Important!** We will be doing some simulations in this seminar; set a seed to make sure your work is reproducible. 

```{r}
set.seed(42)
```


# 1-d smooth regression example

## Example data

Take the standard 1-d regression problem: 

$$
Y = f(X) + \varepsilon
$$

**Task**: we will take a setting in which the conditional expectation $E(Y|X=x)$ is a non-linear function, and will attempt to recover this function using OLS. We will use: 

$$
E(Y \mid X = x ) = x \sin \left ( \frac{1}{x^2} \right )
$$

```{r}
ff <- function(x) x * sin(1/x**2)
```

Write some R code to simulate data from this model when $\varepsilon \sim N(0, \sigma^2)$, then plot your data with a loess smoother using `ggplot`. Experiment with: 

  * Changing the noise level $\sigma$
  * Changing the sample size `nn`

```{r}
nn <- 100
noise_level <- .01

x <- rbeta(nn, 1, 3) 
y <- ff(x) + rnorm(nn, sd = noise_level)

training_data <- data.frame(x = x, y = y)

ggplot(training_data, aes(x, y)) + 
  geom_point() +
  geom_smooth()
```

We would like to fit a linear model to our data which captures the face that the conditional expectation function is non-linear in $x$. Try regressing $y$ on a polynomial transform of $x$.


```{r}
lm_poly <- lm(y ~ poly(x, 5), data = training_data)
```

To visualize our fit it is useful to generate an evenly spaced grid of points from the data. We can do this with the `data_grid` function in the `modelr` library: 

```{r}
x_grid <- data_grid(
  training_data,
  x = seq(from = 0, to = 1, length.out = 500)
)

augment(lm_poly, newdata = x_grid) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_line(aes(y = .fitted)) + 
  geom_point(data = training_data)
```

Two problems are evident: 

* **Problem 1:** to appropriate the conditional expectation well we will need a very
high degree polynomial, for very 

* **Problem 2:** polynomials are unbounded, when we exceed the bounds of the data the polynomial will explode to $\pm \infty$. 

## Gradient descent

Goal: implement gradient descent and use it to solve for the coefficients of the above linear model. We will be evaluating our results multiple times, so write the following functions: 

1. A function which takes in a vector of loss values and returns a plot
2. A function which takes in three vectors of parameters (truth, estimate, OLS) and returns a data frame. 

```{r}

plot_loss <- function(loss_values, steps_taken)
{
  #' Plot loss function by steps taken
  #'
  #'@param loss_values vector
  #'@param steps_taken vector
  
  ggplot(data.frame(loss_values = loss_values, steps_taken = steps_taken), 
       aes(y = loss_values, x = steps_taken)) + 
  geom_line()
  
}

compare_estimates <- function(true_param, estimated_param, X, y)
{
  #' Compares own estimator with the truth and with OLS
  #'
  #'@param true_param vector, dim = (p x 1) 
  #'@param estimated_param vector, dim = (p x 1)
  #'@param X matrix, dim = (n x p)
  #'@param y vector, dim = (n x 1)
  
  data.frame(
  true_beta = beta, 
  descent_beta = next_beta, 
  OLS_beta = coef(lm(y ~ X -1))
  )
}

```


### Step 1: write functions to output the least squares loss and its gradient

Recall: the least squares loss function is as follows: 

$$
L(\beta) = \left \| Y - X \beta \right \|_2^2 = (Y - X \beta)'(Y - X \beta)  
$$

Therefore: the gradient may be written as follows: 

$$
\nabla L(\beta) =  -2X' (Y - X \beta)
$$

```{r}
least_squares_loss <- function(X, Y, beta)
{
  #' Returns least squares loss
  #'
  #'@param X matrix, dim = (n x p)
  #'@param y vector, dim = (n x 1)
  #'@param beta vector, dim = (p x 1)

  sum((Y - X %*% beta)**2)
}

least_squares_gradient <- function(X,Y, beta)
{
  #' Returns least squares gradient
  #'
  #'@param X matrix, dim = (n x p)
  #'@param y vector, dim = (n x 1)
  #'@param beta vector, dim = (p x 1)

  -2 * t(X) %*% (Y - X %*% beta)
}
```

Now simulate some data and check that your function behaves as it you expect. In particular you should check: 

* The dimensions of the output
* The values of the outputs

```{r}
nn <- 100; pp <- 10

X <- matrix(runif(nn*pp), nrow = nn)
beta <- rpois(pp, lambda = 3)
y <- X %*% beta + rnorm(nn)

least_squares_loss(X,y,beta)
least_squares_gradient(X,y,beta)
```

### Step 2: write a loop to take multiple steps in the direction of the negative gradient, keeping the step size fixed

```{r}
step_size <- .001
init_step <- .001
max_steps <- 25

loss_values <- c()
all_steps <- 1:2
n_steps <- 2

beta_0 <- rep(0, pp)

prev_loss <- least_squares_loss(X, y, beta_0)
prev_grad <- least_squares_gradient(X,y, beta_0)

next_beta <- beta_0 - init_step * prev_grad
next_loss <- least_squares_loss(X,y, next_beta)

prev_beta <- next_beta
loss_values <- c(prev_loss, next_loss)

while (n_steps < max_steps) 
{
  next_grad <- least_squares_gradient(X,y, prev_beta)
  next_beta <- prev_beta - step_size * next_grad
  
  prev_beta <- next_beta 
  prev_loss <- next_loss
  
  loss_values <- c(loss_values, next_loss)
  n_steps <- n_steps + 1
  all_steps <- c(all_steps, n_steps)
  next_loss <- least_squares_loss(X, y, next_beta)
}

```

Inspect visually how the value of your loss function changes over the iterations. Also, check how close the parameter obtained through optimization is to the true parameter as well as to the estimate we would have obtained using the `lm` function (i.e. via OLS).

```{r}
plot_loss(loss_values, all_steps)

compare_estimates(beta, next_beta, X, y)
```


### Step 3: write a function to step in the direction of the negative gradient until the loss function no longer decreases by a certain amount, keeping step size fixed

```{r}
step_size <- .001
init_step <- .001
min_decrease <- 1
max_steps <- 1000

loss_values <- c()
all_steps <- 1:2
n_steps <- 2

beta_0 <- rep(0, pp)

prev_loss <- least_squares_loss(X, y, beta_0)
prev_grad <- least_squares_gradient(X,y, beta_0)

next_beta <- beta_0 - init_step * prev_grad
next_loss <- least_squares_loss(X,y, next_beta)

prev_beta <- next_beta
loss_values <- c(prev_loss, next_loss)

while (prev_loss - next_loss > min_decrease & n_steps < max_steps) 
{
  next_grad <- least_squares_gradient(X,y, prev_beta)
  next_beta <- prev_beta - step_size * next_grad
  
  prev_beta <- next_beta 
  prev_loss <- next_loss
  
  loss_values <- c(loss_values, next_loss)
  n_steps <- n_steps + 1
  all_steps <- c(all_steps, n_steps)
  next_loss <- least_squares_loss(X, y, next_beta)
}
```

Again, inspect visually how your loss function behaves and check the value of your parameter estimate. 

```{r}
plot_loss(loss_values, all_steps)

compare_estimates(beta, next_beta, X, y)
```


### Step 5: use the Barzilai-Borwein method to choose step size

See https://en.wikipedia.org/wiki/Gradient_descent

$$
\gamma_n = \frac{\left | (\beta_n - \beta_{n-1})'(\nabla L(\beta_n) - \nabla L(\beta_{n-1})) \right |}{\left \| \nabla L(\beta_n) - \nabla L(\beta_{n-1}) \right \|_2^2}
$$

First, implement a function for finding the step size: 

```{r}
Barzilai_Borwein_step <- function(next_beta, prev_beta, next_grad, prev_grad)
{
  #' Computes step size using the Barzilai-Borwein method
  #'
  #'@param next_beta vector, dim = (p x 1)
  #'@param prev_beta vector, dim = (p x 1)
  #'@param next_grad vector, dim = (p x 1)
  #'@param prev_grad vector, dim = (p x 1)
  
  numerator <- (next_beta - prev_beta) * (next_grad - prev_grad) %>%
    sum() %>% 
    abs()
  
  denominator <- (next_grad - prev_grad) ** 2 %>% sum() 
    
  return(numerator / denominator)
}
```

Now modify the code from the previous section so that the step size is adjusted with each iteration

```{r}
step_size <- .001
init_step <- .001
min_decrease <- 1
max_steps <- 1000

loss_values <- c()
all_steps <- 1:2
n_steps <- 2

beta_0 <- rep(0, pp)

prev_loss <- least_squares_loss(X, y, beta_0)
prev_grad <- least_squares_gradient(X,y, beta_0)

next_beta <- beta_0 - init_step * prev_grad
next_loss <- least_squares_loss(X,y, next_beta)

prev_beta <- next_beta
loss_values <- c(prev_loss, next_loss)

while (prev_loss - next_loss > min_decrease & n_steps < max_steps) 
{
  next_grad <- least_squares_gradient(X,y, prev_beta)
  loc_step <- Barzilai_Borwein_step(next_beta, prev_beta, next_grad, prev_grad)
  next_beta <- prev_beta - loc_step * next_grad
  
  prev_beta <- next_beta 
  prev_loss <- next_loss
  
  loss_values <- c(loss_values, next_loss)
  n_steps <- n_steps + 1
  all_steps <- c(all_steps, n_steps)
  next_loss <- least_squares_loss(X, y, next_beta)
}
```

Again, inspect visually how your loss function behaves and check the value of your parameter estimate. 

```{r}
plot_loss(loss_values, all_steps)

compare_estimates(beta, next_beta, X, y)
```


### Step 6: keeping a fixed step size implement (mini-batch?) stochastic gradient descent

```{r}
step_size <- .001
init_step <- .001
min_decrease <- 1
max_steps <- 1000

batch_size <- 2

loss_values <- c()
all_steps <- 1:2
n_steps <- 2

beta_0 <- rep(0, pp)

prev_loss <- least_squares_loss(X, y, beta_0)
prev_grad <- least_squares_gradient(X,y, beta_0)

next_beta <- beta_0 - init_step * prev_grad
next_loss <- least_squares_loss(X,y, next_beta)

prev_beta <- next_beta
loss_values <- c(prev_loss, next_loss)

while (prev_loss - next_loss > min_decrease & n_steps < max_steps) 
{
  selected_ind <- sample(1:pp, batch_size)
  X_batch <- X[selected_ind,] 
  y_batch <- y[selected_ind]
  
  next_grad <- least_squares_gradient(X_batch, y_batch, prev_beta)
  next_beta <- prev_beta - step_size * next_grad
  
  prev_beta <- next_beta 
  prev_loss <- next_loss
  
  loss_values <- c(loss_values, next_loss)
  n_steps <- n_steps + 1
  all_steps <- c(all_steps, n_steps)
  next_loss <- least_squares_loss(X, y, next_beta)
}
```

Again, inspect visually how your loss function behaves and check the value of your parameter estimate. 

```{r}
plot_loss(loss_values, all_steps)

compare_estimates(beta, next_beta, X, y)
```

# Classification example

## Generate data

Which predictors have nonzero coefficients?

```{r}
nn <- 100 
pp <- 10
sparsity <- 2

X <- matrix(rnorm(nn*pp), nrow = nn)

beta <- c(rep(1, sparsity), rep(0, pp - sparsity))

mu <- X %*% beta
px <- exp(mu)/(1+exp(mu))
y <- rbinom(nn, 1, px)
```

## Implement gradient descent for the logistic regression

First implement the logistic regression loss function, that is: 

$$
\ell (\beta) = - \sum_{i=1}^n \left \{ y_i \log \left ( \frac{1}{1+e^{-x_i'\beta}} \right ) + (1-y_i) \log \left ( 1 - \frac{1}{1+e^{-x_i'\beta}}\right ) \right \}
$$

```{r}
logistic_log_likelihood <- function(X, y, beta)
{
  #' Computes the logistic log-likelihood function
  #'
  #'@param y vector, dim = (n x 1)
  #'@param X matrix, dim = (n x p) 
  #'@param beta vector, dim = (p x 1)

  term_1 <- y * log(1/(1+exp(-(X %*% beta)))) 
    
  term_2 <- (1-y) * log(1/(1+exp(-(X %*% beta))))
  
  return(
    - (term_1 + term_2) %>% sum()
  )
}
```

Next, write a function for finding the derivative of a function numberically. Recall the definition of a derivative... 

$$
f'(x) = \lim_{\Delta \rightarrow 0} \frac{f(x+\Delta) - f(x)}{\Delta}
$$

```{r}
secant_numerical_derivative <- function(X, y, beta, ff)
{
  #' Finds vector of partial derivatives using the secant method
  #'
  #'@param X matrix, dim = (n x p)
  #'@param y vector, dim = (n x 1)
  #'@param beta vector, dim = (p x 1)
  #'@param ff function, (X, y, beta) \mapsto R^+
  
  pp <- length(beta_0)
  eps <- 1
  numeric_derivative <- numeric(pp)

  for (ii in 1:pp)
    {
    beta_plus_eps <- beta_0 + c(rep(0,ii-1), eps, rep(0,pp-ii))
    beta_minus_eps <- beta_0 + c(rep(0,ii-1), -eps, rep(0,pp-ii))
  
    numeric_derivative[ii] <- (ff(X, y, beta_plus_eps) - ff(X, y, beta_minus_eps)) / (2 * eps)
  }

  return(numeric_derivative)
}
```

Use the functions we have just defined to implement gradient descent for the logistic regression problem. 

```{r}
step_size <- .001
init_step <- .0001
min_decrease <- .01
max_steps <- 1000

loss_values <- c()
all_steps <- 1:2
n_steps <- 2

beta_0 <- runif(pp)

prev_loss <- logistic_log_likelihood(X, y, beta_0)
prev_grad <- secant_numerical_derivative(X, y, beta_0, ff = logistic_log_likelihood)

next_beta <- beta_0 - init_step * prev_grad
next_loss <- logistic_log_likelihood(X, y, next_beta)

prev_beta <- next_beta
loss_values <- c(prev_loss, next_loss)

while (prev_loss - next_loss > min_decrease & n_steps < max_steps) 
{
  next_grad <- secant_numerical_derivative(X, y, prev_beta, ff = logistic_log_likelihood)
  next_beta <- prev_beta - step_size * next_grad
  
  prev_beta <- next_beta 
  prev_loss <- next_loss
  
  loss_values <- c(loss_values, next_loss)
  n_steps <- n_steps + 1
  all_steps <- c(all_steps, n_steps)
  next_loss <- logistic_log_likelihood(X, y, next_beta)
}
```

Plot your loss function: 

```{r}
plot_loss(loss_values, all_steps)
```

### Check answer with results from `glm` (if possible, when p < n)

```{r}
data.frame(
  true_beta = beta, 
  glm_beta = glm(y ~ X - 1, family = "binomial")$coef, 
  descent_beta = next_beta
)
```

# Extra practice

## High dimensional regression and SGD

Generate data some high dimensional data: 

```{r}
nn <- 100
pp <- 200
sparsity <- 3

X <- matrix(rnorm(nn*pp), nrow = nn)
beta <- c(rep(1, sparsity), rep(0, pp - sparsity))
y <- X %*% beta

train_hd <- data.frame(y = y, x = x)
```

```{r}
train_hd %>% 
  select(y, num_range("x.", 1:6)) %>%
  ggpairs(progress = F)
```


Question: does it converge? There is no unique solution for least squares in high dimensional regression.

### Even more extra practice! 

- Modify the least squares loss function by adding a penalty on the coefficients, e.g. loss = MSE + L*beta^2, where L is a non-negative constant. Compute the new gradient and implement SGD with this loss function. Try various values of L on the same dataset, and implement "tuning" to find the best value of L 

- Implement SGD for logistic and hinge loss functions, and try it out on some examples of data that you generate where you know the true answer

# References

- https://mlstory.org/optimization.html for gradient descent, stochastic gradient descent, SGD quick start guide
- https://en.wikipedia.org/wiki/Numerical_differentiation for symmetric difference quotient, step size
- Type `.Machine` in the console and look for `double.eps`